{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the MNIST datasets - 60000 images for training and 10000 images for testing\n",
    "(imageTrainRaw, labelTrainRaw), (imageTestRaw, labelTestRaw) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Splitting the training set into 2: 55000 images for training and 5000 images for validation\n",
    "imageTestRaw = imageTestRaw[:]\n",
    "labelTestRaw = labelTestRaw[:]\n",
    "imageValidRaw = imageTrainRaw[55000:]\n",
    "labelValidRaw = labelTrainRaw[55000:]\n",
    "imageTrainRaw = imageTrainRaw[:55000]\n",
    "labelTrainRaw = labelTrainRaw[:55000]\n",
    "\n",
    "def printArr(idx):\n",
    "    to_write = \"\"\n",
    "    for i in range(28):\n",
    "        for j in range(28):\n",
    "            if(len(str(imageTrainRaw[idx][i][j])) == 1):\n",
    "                to_write += str(imageTrainRaw[idx][i][j]) + \"    \"\n",
    "            elif(len(str(imageTrainRaw[idx][i][j])) == 2):\n",
    "                to_write += str(imageTrainRaw[idx][i][j]) + \"   \"\n",
    "            else:\n",
    "                to_write += str(imageTrainRaw[idx][i][j]) + \"  \"\n",
    "        to_write += \"\\n\"\n",
    "    to_write += f\"\\nDigit: {labelTrainRaw[idx]}\"\n",
    "    with open(\"./test/imageArr.txt\", \"w\") as file:\n",
    "        file.write(to_write)\n",
    "\n",
    "printArr(idx = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting an image\n",
    "def plotImage(idx):\n",
    "    pyplot.title(f\"Digit {labelTrainRaw[idx]}\")\n",
    "    pyplot.imshow(imageTrainRaw[idx], cmap='gray')\n",
    "    pyplot.savefig(\"./test/digitGrey.jpg\")\n",
    "    \n",
    "plotImage(idx = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing data (normalizing image arrays and 1-hot encoding label arrays)\n",
    "imageTest = np.reshape(imageTestRaw, (10000, 784))\n",
    "imageTest = preprocessing.normalize(imageTest, norm = \"max\")\n",
    "labelTest = tf.keras.utils.to_categorical(labelTestRaw, num_classes=10)\n",
    "\n",
    "imageValid = np.reshape(imageValidRaw, (5000, 784))\n",
    "imageValid = preprocessing.normalize(imageValid, norm = \"max\")\n",
    "labelValid = tf.keras.utils.to_categorical(labelValidRaw, num_classes=10)\n",
    "\n",
    "imageTrain = np.reshape(imageTrainRaw, (55000, 784))\n",
    "imageTrain = preprocessing.normalize(imageTrain, norm=\"max\")\n",
    "labelTrain = tf.keras.utils.to_categorical(labelTrainRaw, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the neural network layer dimension and hypermeters \n",
    "iter = 20000      # Number of iterations\n",
    "sizeTrain = 55000 # Images in train set\n",
    "sizeValid = 5000  # Images in train set\n",
    "batchSize = 50    # Batch size\n",
    "etaInit = 0.05    # Initial learning rate\n",
    "etaDecay = 1      # Learning rate decay\n",
    "dInput = 784      # Input layer (28x28 pixels)\n",
    "dHidden = 400     # Hidden layer (16x16 pixel)\n",
    "dOutput = 10      # Output layer (0-9 digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU activation for hidden layer\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Softmax activation for output layer\n",
    "def softmax(x):\n",
    "    c = np.max(x, axis = 0)\n",
    "    x -= c\n",
    "    e = np.exp(x)\n",
    "    return e / sum(e)\n",
    "\n",
    "# Calculating loss value using cross entropy\n",
    "def flossSGD(yCal, yLabel):\n",
    "    result = sum(- np.log(yCal)[idx] * yLabel[idx] for idx in range(10))\n",
    "    return result\n",
    "\n",
    "def floss(yCal, yLabel):\n",
    "    result = -np.sum(np.log(yCal) * yLabel) / batchSize\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter 0] Loss: 2.3000295168263096\n",
      "[Iter 2] Loss: 2.2730865689092896\n",
      "[Iter 4] Loss: 2.240529591173707\n",
      "[Iter 5] Loss: 2.2130775679666597\n",
      "[Iter 23] Loss: 2.185545266203702\n",
      "[Iter 37] Loss: 2.172640223892534\n",
      "[Iter 78] Loss: 2.131641019350424\n",
      "[Iter 85] Loss: 2.1121898645474606\n",
      "[Iter 87] Loss: 2.1076070235671107\n",
      "[Iter 89] Loss: 2.1036526602259324\n",
      "[Iter 90] Loss: 2.0280071170442158\n",
      "[Iter 95] Loss: 1.9509223945053713\n",
      "[Iter 103] Loss: 1.8347217949987096\n",
      "[Iter 114] Loss: 1.7656639518278023\n",
      "[Iter 120] Loss: 1.7101988634768681\n",
      "[Iter 123] Loss: 1.6066889686132084\n",
      "[Iter 125] Loss: 1.5676852233868588\n",
      "[Iter 133] Loss: 1.5087174897590432\n",
      "[Iter 134] Loss: 1.4721103857259004\n",
      "[Iter 139] Loss: 1.432014320048273\n",
      "[Iter 141] Loss: 1.43009758180914\n",
      "[Iter 145] Loss: 1.391103722028061\n",
      "[Iter 146] Loss: 1.3156591496057701\n",
      "[Iter 155] Loss: 1.2527904445663105\n",
      "[Iter 162] Loss: 1.1736150880757659\n",
      "[Iter 166] Loss: 1.1621694204476567\n",
      "[Iter 167] Loss: 1.0880212349668321\n",
      "[Iter 176] Loss: 1.0731734014932597\n",
      "[Iter 181] Loss: 1.0651719547883594\n",
      "[Iter 182] Loss: 0.9709171586535297\n",
      "[Iter 189] Loss: 0.900760628385093\n",
      "[Iter 201] Loss: 0.8102941229596123\n",
      "[Iter 228] Loss: 0.6707529864520105\n",
      "[Iter 243] Loss: 0.6589707912381689\n",
      "[Iter 256] Loss: 0.6366400331058746\n",
      "[Iter 263] Loss: 0.607162781762096\n",
      "[Iter 281] Loss: 0.5949752078910663\n",
      "[Iter 287] Loss: 0.5949750342586292\n",
      "[Iter 290] Loss: 0.5367006684139\n",
      "[Iter 291] Loss: 0.46147586720767814\n",
      "[Iter 296] Loss: 0.4092339927682275\n",
      "[Iter 332] Loss: 0.3591706335719405\n",
      "[Iter 416] Loss: 0.35903367739403325\n",
      "[Iter 445] Loss: 0.3506909578536168\n",
      "[Iter 473] Loss: 0.3219006293604076\n",
      "[Iter 474] Loss: 0.31180079591556054\n",
      "[Iter 484] Loss: 0.2675309708157152\n",
      "[Iter 533] Loss: 0.25686677126946367\n",
      "[Iter 553] Loss: 0.2252479784704897\n",
      "[Iter 683] Loss: 0.13816438647756224\n",
      "[Iter 1143] Loss: 0.12874085709454344\n",
      "[Iter 1275] Loss: 0.10780229959027945\n",
      "[Iter 1765] Loss: 0.06990768839489206\n",
      "[Iter 2515] Loss: 0.06669543229175048\n",
      "[Iter 2619] Loss: 0.047678269887374114\n",
      "[Iter 4329] Loss: 0.0419560918840728\n",
      "[Iter 4620] Loss: 0.03488411898744741\n",
      "[Iter 4921] Loss: 0.033906839521266045\n",
      "[Iter 5657] Loss: 0.03198941114507671\n",
      "[Iter 6430] Loss: 0.030096550585012073\n",
      "[Iter 6507] Loss: 0.02788163878881142\n",
      "[Iter 6734] Loss: 0.02230458224376859\n",
      "[Iter 7229] Loss: 0.01952095070593467\n",
      "[Iter 7765] Loss: 0.017830351366869238\n",
      "[Iter 8085] Loss: 0.017623967210895382\n",
      "[Iter 8132] Loss: 0.013715223778927931\n",
      "[Iter 9578] Loss: 0.007740826236491718\n",
      "[Iter 13699] Loss: 0.006583807095232597\n",
      "[Iter 14158] Loss: 0.006197880801198312\n",
      "[Iter 15009] Loss: 0.006152536723207413\n",
      "[Iter 16899] Loss: 0.004298839185110251\n",
      "[Iter 17202] Loss: 0.004295143838997865\n",
      "[Iter 18957] Loss: 0.004002592771515162\n",
      "[Iter 19306] Loss: 0.0028470794219049777\n"
     ]
    }
   ],
   "source": [
    "# Initializing weights, biases and eta\n",
    "w1 = 0.01 * np.random.rand(dInput, dHidden)\n",
    "b1 = 0.01 * np.random.rand(dHidden, batchSize)\n",
    "w2 = 0.01 * np.random.rand(dHidden, dOutput)\n",
    "b2 = 0.01 * np.random.rand(dOutput, batchSize)\n",
    "eta = etaInit\n",
    "\n",
    "lossPrev = 100000\n",
    "for n in range(iter + 1):\n",
    "    idxList = np.random.randint(sizeTrain, size=batchSize)\n",
    "    # Feedforward\n",
    "    x = imageTrain[idxList].T\n",
    "    z1 = np.dot(w1.T, x) + b1\n",
    "    a1 = relu(z1)\n",
    "    z2 = np.dot(w2.T, a1) + b2\n",
    "    a2 = softmax(z2)\n",
    "    yCal = a2 \n",
    "    yLabel = labelTrain[idxList].T\n",
    "    loss = floss(yCal, yLabel) \n",
    "    if(loss < lossPrev):\n",
    "        lossPrev = loss\n",
    "        w1Res = w1\n",
    "        b1Res = b1\n",
    "        w2Res = w2\n",
    "        b2Res = b2\n",
    "        print(f\"[Iter {n}] Loss: {loss}\")\n",
    "        \n",
    "    # Backpropagation\n",
    "    e2 = (yCal - yLabel) / batchSize # gradient of softmax using cross entropy\n",
    "    dw2 = np.dot(a1, e2.T)\n",
    "    db2 = sum(e2)\n",
    "    e1 = np.dot(w2, e2)\n",
    "    e1[z1 <= 0] = 0 # gradient of ReLU\n",
    "    dw1 = np.dot(x, e1.T)\n",
    "    db1 = sum(e1)\n",
    "\n",
    "    # Updating weights and biases\n",
    "    w1 += -eta * dw1\n",
    "    b1 += -eta * db1\n",
    "    w2 += -eta * dw2\n",
    "    b2 += -eta * db2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'arrange'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [33], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m batch \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m10000\u001b[39m:\n\u001b[1;32m----> 4\u001b[0m     idxList \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marrange\u001b[49m(batch, batch \u001b[38;5;241m+\u001b[39m batchSize, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      5\u001b[0m     x \u001b[38;5;241m=\u001b[39m imageTest[idxList]\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m      6\u001b[0m     z1 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(w1Res\u001b[38;5;241m.\u001b[39mT, x) \u001b[38;5;241m+\u001b[39m b1Res\n",
      "File \u001b[1;32mc:\\Users\\TheNM\\Downloads\\Coding\\ai-project\\venv\\lib\\site-packages\\numpy\\__init__.py:311\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(attr)\u001b[0m\n\u001b[0;32m    308\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mtesting\u001b[39;00m \u001b[39mimport\u001b[39;00m Tester\n\u001b[0;32m    309\u001b[0m     \u001b[39mreturn\u001b[39;00m Tester\n\u001b[1;32m--> 311\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mmodule \u001b[39m\u001b[39m{!r}\u001b[39;00m\u001b[39m has no attribute \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    312\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39m{!r}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39m__name__\u001b[39m, attr))\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'arrange'"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "batch = 0\n",
    "while batch < 10000:\n",
    "    idxList = np.arange(batch, batch + batchSize, 1)\n",
    "    x = imageTest[idxList].T\n",
    "    z1 = np.dot(w1Res.T, x) + b1Res\n",
    "    a1 = relu(z1)\n",
    "    z2 = np.dot(w2Res.T, a1) + b2Res\n",
    "    a2 = softmax(z2)\n",
    "    yLabel = labelTest[idxList]\n",
    "    prediction = np.argmax(a2.T, axis=1)\n",
    "    answer = np.argmax(yLabel, axis = 1)\n",
    "    for n in range(50):\n",
    "        if(prediction[n] == answer[n]):\n",
    "            count += 1\n",
    "    batch += batchSize\n",
    "\n",
    "print(count / 10000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f2ec3a27b206c5c715624a547105f4453c6ed312d93e4b85e955c3b1a3db3dae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
