{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the MNIST datasets - 60000 images for training and 10000 images for testing\n",
    "(imageTrainRaw, labelTrainRaw), (imageTestRaw, labelTestRaw) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Splitting the training set into 2: 55000 images for training and 5000 images for validation\n",
    "imageTestRaw = imageTestRaw[:]\n",
    "labelTestRaw = labelTestRaw[:]\n",
    "imageValidRaw = imageTrainRaw[55000:]\n",
    "labelValidRaw = labelTrainRaw[55000:]\n",
    "imageTrainRaw = imageTrainRaw[:55000]\n",
    "labelTrainRaw = labelTrainRaw[:55000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting an image\n",
    "def plotImage(idx):\n",
    "    pyplot.title(f\"Digit {labelTrainRaw[idx]}\")\n",
    "    pyplot.imshow(imageTrainRaw[idx], cmap='gray')\n",
    "    pyplot.savefig(\"./test/digitGrey.jpg\")\n",
    "    \n",
    "plotImage(idx = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing data (normalizing image arrays and 1-hot encoding label arrays)\n",
    "imageTest = np.reshape(imageTestRaw, (10000, 784))\n",
    "imageTest = preprocessing.normalize(imageTest, norm = \"max\")\n",
    "labelTest = tf.keras.utils.to_categorical(labelTestRaw, num_classes=10)\n",
    "\n",
    "imageValid = np.reshape(imageValidRaw, (5000, 784))\n",
    "imageValid = preprocessing.normalize(imageValid, norm = \"max\")\n",
    "labelValid = tf.keras.utils.to_categorical(labelValidRaw, num_classes=10)\n",
    "\n",
    "imageTrain = np.reshape(imageTrainRaw, (55000, 784))\n",
    "imageTrain = preprocessing.normalize(imageTrain, norm=\"max\")\n",
    "labelTrain = tf.keras.utils.to_categorical(labelTrainRaw, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the neural network layer dimension and hypermeters \n",
    "iter = 10000    # Number of iterations\n",
    "size = 55000    # Number of inputs\n",
    "batchSize = 50  # Size of mini-batch GD\n",
    "etaInit = 0.01  # Initial learning rate\n",
    "etaDecay = 1    # Learning rate decay\n",
    "dInput = 784    # Input layer (28x28 pixels)\n",
    "dHidden = 400   # Hidden layer (16x16 pixel)\n",
    "dOutput = 10    # Output layer (0-9 digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU activation for hidden layer\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Softmax activation for output layer\n",
    "def softmax(x):\n",
    "    c = np.max(x, axis = 0, keepdims = True)\n",
    "    x -= c\n",
    "    e = np.exp(x)\n",
    "    return e / sum(e)\n",
    "\n",
    "# Calculating loss value using cross entropy\n",
    "def floss(yCal, yLabel):\n",
    "    result = -np.sum(np.log(yCal) * yLabel) / batchSize\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter 0] Loss: 2.3019720946968416\n",
      "[Iter 2] Loss: 2.2967432567937265\n",
      "[Iter 7] Loss: 2.261041045206833\n",
      "[Iter 9] Loss: 2.2192570737106854\n",
      "[Iter 133] Loss: 2.189421644630764\n",
      "[Iter 273] Loss: 2.172549405973304\n",
      "[Iter 286] Loss: 2.1585551972114705\n",
      "[Iter 359] Loss: 2.1356571720989495\n",
      "[Iter 360] Loss: 2.1000154194118266\n",
      "[Iter 397] Loss: 2.0975715746248325\n",
      "[Iter 407] Loss: 2.0783440667594575\n",
      "[Iter 429] Loss: 2.055639752109071\n",
      "[Iter 447] Loss: 2.0502618646580952\n",
      "[Iter 448] Loss: 2.0483972582901937\n",
      "[Iter 453] Loss: 2.045354550947013\n",
      "[Iter 461] Loss: 2.0381022787152863\n",
      "[Iter 462] Loss: 2.027288531007217\n",
      "[Iter 467] Loss: 2.0272190265351147\n",
      "[Iter 469] Loss: 2.026723818578662\n",
      "[Iter 471] Loss: 2.009725145066069\n",
      "[Iter 475] Loss: 1.9791240911007508\n",
      "[Iter 485] Loss: 1.978566597082701\n",
      "[Iter 487] Loss: 1.967012177072063\n",
      "[Iter 492] Loss: 1.9561647137122815\n",
      "[Iter 497] Loss: 1.9252220125286983\n",
      "[Iter 500] Loss: 1.8527615578503571\n",
      "[Iter 512] Loss: 1.8515481201668769\n",
      "[Iter 522] Loss: 1.8356696696562622\n",
      "[Iter 544] Loss: 1.8136624730550899\n",
      "[Iter 546] Loss: 1.7972943926409364\n",
      "[Iter 554] Loss: 1.7900229130344607\n",
      "[Iter 563] Loss: 1.785963177729726\n",
      "[Iter 566] Loss: 1.7684286919941896\n",
      "[Iter 575] Loss: 1.7236853004531594\n",
      "[Iter 579] Loss: 1.7236130026648384\n",
      "[Iter 588] Loss: 1.6498242743764093\n",
      "[Iter 607] Loss: 1.6020552219189759\n",
      "[Iter 632] Loss: 1.596354138695321\n",
      "[Iter 637] Loss: 1.5830025565305959\n",
      "[Iter 643] Loss: 1.5511051413545756\n",
      "[Iter 645] Loss: 1.5265180594425956\n",
      "[Iter 653] Loss: 1.4858781285000362\n",
      "[Iter 660] Loss: 1.4840739420131737\n",
      "[Iter 672] Loss: 1.4832414804534542\n",
      "[Iter 677] Loss: 1.458076218028807\n",
      "[Iter 681] Loss: 1.4474407047602391\n",
      "[Iter 686] Loss: 1.4451318544679344\n",
      "[Iter 688] Loss: 1.444071251864047\n",
      "[Iter 700] Loss: 1.413386277487234\n",
      "[Iter 707] Loss: 1.3817603756800287\n",
      "[Iter 716] Loss: 1.3451214074459017\n",
      "[Iter 726] Loss: 1.290534208881329\n",
      "[Iter 730] Loss: 1.2152844854496179\n",
      "[Iter 738] Loss: 1.2116698833562298\n",
      "[Iter 753] Loss: 1.1797341795264273\n",
      "[Iter 777] Loss: 1.1529108674002735\n",
      "[Iter 795] Loss: 1.1224801614588067\n",
      "[Iter 802] Loss: 1.0711772886527446\n",
      "[Iter 824] Loss: 1.063792656479718\n",
      "[Iter 834] Loss: 1.0407198305769472\n",
      "[Iter 856] Loss: 1.0400093670311348\n",
      "[Iter 871] Loss: 0.995077219028463\n",
      "[Iter 901] Loss: 0.891809328505656\n",
      "[Iter 953] Loss: 0.8598539604072489\n",
      "[Iter 997] Loss: 0.8143684426560551\n",
      "[Iter 1083] Loss: 0.7745416295943897\n",
      "[Iter 1100] Loss: 0.7428452764558157\n",
      "[Iter 1109] Loss: 0.6550805458320152\n",
      "[Iter 1166] Loss: 0.6517408992457021\n",
      "[Iter 1195] Loss: 0.6300079769736927\n",
      "[Iter 1210] Loss: 0.5998198395192812\n",
      "[Iter 1221] Loss: 0.5862654266404597\n",
      "[Iter 1246] Loss: 0.5420382577693749\n",
      "[Iter 1381] Loss: 0.5252201644187253\n",
      "[Iter 1384] Loss: 0.5179439976255265\n",
      "[Iter 1388] Loss: 0.5161602631904191\n",
      "[Iter 1398] Loss: 0.4878731935552702\n",
      "[Iter 1430] Loss: 0.46047286280626964\n",
      "[Iter 1491] Loss: 0.3948248272077071\n",
      "[Iter 1514] Loss: 0.391691852358789\n",
      "[Iter 1560] Loss: 0.3662989451992483\n",
      "[Iter 1706] Loss: 0.33298167422828384\n",
      "[Iter 1885] Loss: 0.29799850960633634\n",
      "[Iter 1995] Loss: 0.27809205818069366\n",
      "[Iter 2221] Loss: 0.20027593722387518\n",
      "[Iter 2923] Loss: 0.17139474637232177\n",
      "[Iter 3050] Loss: 0.16498395681218525\n",
      "[Iter 3220] Loss: 0.16179766075003138\n",
      "[Iter 3534] Loss: 0.15624610913393902\n",
      "[Iter 4173] Loss: 0.1543270228753992\n",
      "[Iter 4222] Loss: 0.15170522568818295\n",
      "[Iter 4309] Loss: 0.13369317784029633\n",
      "[Iter 4901] Loss: 0.11697182922952583\n",
      "[Iter 5067] Loss: 0.08747963575803308\n",
      "[Iter 7436] Loss: 0.08304839242509285\n",
      "[Iter 8240] Loss: 0.07843145470877295\n",
      "[Iter 9243] Loss: 0.0685628689588549\n",
      "[Iter 9938] Loss: 0.05487643757565566\n"
     ]
    }
   ],
   "source": [
    "# Initializing weights, biases and eta\n",
    "w1 = 0.01 * np.random.rand(dInput, dHidden)\n",
    "b1 = 0.01 * np.random.rand(dHidden, batchSize)\n",
    "w2 = 0.01 * np.random.rand(dHidden, dOutput)\n",
    "b2 = 0.01 * np.random.rand(dOutput, batchSize)\n",
    "eta = etaInit\n",
    "\n",
    "lossPrev = 1000\n",
    "for n in range(iter + 1):\n",
    "    idxList = np.random.randint(55000, size=batchSize)\n",
    "    # Feed forward\n",
    "    x = imageTrain[idxList].T\n",
    "    z1 = np.dot(w1.T, x) + b1\n",
    "    a1 = relu(z1)\n",
    "    z2 = np.dot(w2.T, a1) + b2\n",
    "    a2 = softmax(z2)\n",
    "    yCal = a2 \n",
    "    yLabel = labelTrain[idxList].T\n",
    "    loss = floss(yCal, yLabel) \n",
    "    if(loss < lossPrev):\n",
    "        lossPrev = loss\n",
    "        w1Res = w1\n",
    "        b1Res = b1\n",
    "        w2Res = w2\n",
    "        b2Res = b2\n",
    "        print(f\"[Iter {n}] Loss: {loss}\")\n",
    "        \n",
    "    # Back propagation\n",
    "    e2 = (yCal - yLabel) / batchSize   # gradient of softmax using cross entropy\n",
    "    dw2 = np.dot(a1, e2.T)\n",
    "    db2 = sum(e2)\n",
    "    e1 = np.dot(w2, e2)\n",
    "    e1[z1 <= 0] = 0                    # gradient of ReLU\n",
    "    dw1 = np.dot(x, e1.T)\n",
    "    db1 = sum(e1)\n",
    "\n",
    "    # Updating weights and biases\n",
    "    w1 += -eta * dw1\n",
    "    b1 += -eta * db1\n",
    "    w2 += -eta * dw2\n",
    "    b2 += -eta * db2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9252\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "epoch = 0\n",
    "while epoch < 5000:\n",
    "    idxList = np.arange(epoch, epoch + 50, 1)\n",
    "    x = imageTrain[idxList].T\n",
    "    z1 = np.dot(w1Res.T, x) + b1Res\n",
    "    a1 = relu(z1)\n",
    "    z2 = np.dot(w2Res.T, a1) + b2Res\n",
    "    a2 = softmax(z2)\n",
    "    yLabel = labelTrain[idxList]\n",
    "    prediction = np.argmax(a2.T, axis=1)\n",
    "    answer = np.argmax(yLabel, axis = 1)\n",
    "    for n in range(50):\n",
    "        if(prediction[n] == answer[n]):\n",
    "            count += 1\n",
    "    epoch += 50\n",
    "\n",
    "print(count / 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "idxList = np.arange(0, 50, 1)\n",
    "x = imageTrain[idxList].T\n",
    "z1 = np.dot(w1Res.T, x) + b1Res\n",
    "a1 = relu(z1)\n",
    "z2 = np.dot(w2Res.T, a1) + b2Res\n",
    "a2 = softmax(z2)\n",
    "yLabel = labelTrain[idxList]\n",
    "prediction = np.argmax(a2.T, axis=1)\n",
    "answer = np.argmax(yLabel, axis = 1)\n",
    "for n in range(50):\n",
    "    if(prediction[n] == answer[n]):\n",
    "        count += 1\n",
    "        \n",
    "print(count / 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f2ec3a27b206c5c715624a547105f4453c6ed312d93e4b85e955c3b1a3db3dae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
